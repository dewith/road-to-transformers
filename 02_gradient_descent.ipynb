{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš¦ Loss functions and Gradient Calculations\n",
    "\n",
    "\n",
    "1. Find the gradient of the following function at the point (2, 1): \n",
    "    \n",
    "    $f(x, y) = x^3 - 2xy + y^2$\n",
    "\n",
    "2. Find the derivative of the softmax function with respect to x_i: \n",
    "   \n",
    "   $softmax(x_i) = \\frac{e^{x_i}}{Î£_j e^{x_j}}$, \n",
    "   where $Î£_j$ denotes the sum over all $j$.\n",
    "\n",
    "3. Find the derivative of the mean squared error loss function with respect to the predicted value Å·: \n",
    "   \n",
    "   $MSE(y, Å·) = \\frac{Î£_i (y_i - Å·_i)^2}{n} $, \n",
    "   where $n$ is the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gradient of a Function\n",
    "\n",
    "Given the funtion \n",
    "\n",
    "$f(x, y) = x^3 - 2xy + y^2$\n",
    "\n",
    "The gradient of the function is given by the partial derivatives of the function with respect to x and y.\n",
    "\n",
    "$\\nabla f(x, y) = [\\frac{âˆ‚f}{âˆ‚x}, \\frac{âˆ‚f}{âˆ‚y}]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we calculate the partial derivatives of the function, we get:\n",
    "\n",
    "$\\frac{âˆ‚f}{âˆ‚x} = 3x^2 - 2y$\n",
    "\n",
    "$\\frac{âˆ‚f}{âˆ‚y} = -2x + 2y$\n",
    "\n",
    "Therefore, the gradient of the function is:\n",
    "\n",
    "$\\nabla f(x, y) = [3x^2 - 2y, -2x + 2y]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can find the gradient of the function at the point (2, 1):\n",
    "\n",
    "$\\nabla f(2, 1) = [3(2)^2 - 2(1), -2(2) + 2(1)]$\n",
    "\n",
    "$\\nabla f(2, 1) = [12 - 2, -4 + 2]$\n",
    "\n",
    "$\\nabla f(2, 1) = [10, -2]$\n",
    "\n",
    "This means that at the point (2, 1), the function is increasing most rapidly in the x-direction and decreasing in the y-direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 -2]\n"
     ]
    }
   ],
   "source": [
    "# Analytical gradient\n",
    "point = np.array([2, 1])\n",
    "df_dx = (3 * point[0] ** 2) - (2 * point[1])\n",
    "df_dy = (-2 * point[0]) + (2 * point[1])\n",
    "gradient = np.array([df_dx, df_dy])\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical gradient\n",
    "\n",
    "\n",
    "def fun(x, y):\n",
    "    \"\"\"Function for which to calculate the gradient.\"\"\"\n",
    "    return x**3 - 2 * x * y + y**2\n",
    "\n",
    "\n",
    "def gradient(fun, x, y, h=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate the numerical gradient of a function at a point using\n",
    "    finite difference approximations in a rigorous way.\n",
    "    \"\"\"\n",
    "    df_dx = (fun(x + h, y) - fun(x, y)) / h\n",
    "    df_dy = (fun(x, y + h) - fun(x, y)) / h\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "\n",
    "def gradient_symmetric(fun, x, y, h=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate the numerical gradient of a function at a point\n",
    "    using the symmetric difference quotient.\n",
    "    \"\"\"\n",
    "    df_dx = (fun(x + h, y) - fun(x - h, y)) / (2 * h)\n",
    "    df_dy = (fun(x, y + h) - fun(x, y - h)) / (2 * h)\n",
    "    return np.array([df_dx, df_dy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.000006 -1.999999]\n",
      "[10. -2.]\n"
     ]
    }
   ],
   "source": [
    "print(gradient(fun, x=2, y=1, h=1e-6))\n",
    "print(gradient_symmetric(fun, x=2, y=1, h=1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Derivative of the Softmax Function\n",
    "\n",
    "The softmax function is used in machine learning to convert a vector of real numbers into a probability distribution. The softmax function is defined as:\n",
    "\n",
    "$softmax(x_i) = \\frac{e^{x_i}}{Î£_j e^{x_j}}$\n",
    "\n",
    "To find the derivative of the softmax function with respect to $x_i$, we can use the quotient rule of differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $f(x) = e^{x_i}$ and $g(x) = Î£_j e^{x_j}$, then the softmax function can be written as:\n",
    "\n",
    "$softmax(x_i) = \\frac{f(x_i)}{g(x)}$\n",
    "\n",
    "The derivative of the softmax function with respect to $x_i$ is given by:\n",
    "\n",
    "$\\frac{d}{dx_i} softmax(x_i) = \\frac{g(x)f'(x_i) - f(x_i)g'(x)}{(g(x))^2}$\n",
    "\n",
    "Where $f'(x_i)$ is the derivative of $e^{x_i}$ with respect to $x_i$ and $g'(x)$ is the derivative of $Î£_j e^{x_j}$ with respect to $x_i$.\n",
    "\n",
    "The derivative of $e^{x_i}$ with respect to $x_i$ is simply $e^{x_i}$.\n",
    "\n",
    "The derivative of $Î£_j e^{x_j}$ with respect to $x_i$ is $e^{x_i}$, as the sum is over all $j$ and $x_i$ is one of the terms in the sum.\n",
    "\n",
    "Therefore, the derivative of the softmax function with respect to $x_i$ is:\n",
    "\n",
    "$\\frac{d}{dx_i} softmax(x_i) = \\frac{Î£_j e^{x_j}e^{x_i} - e^{x_i}e^{x_i}}{(Î£_j e^{x_j})^2}$\n",
    "\n",
    "$\\frac{d}{dx_i} softmax(x_i) = \\frac{e^{x_i}Î£_j e^{x_j} - e^{2x_i}}{(Î£_j e^{x_j})^2}$\n",
    "\n",
    "$\\frac{d}{dx_i} softmax(x_i) = \\frac{e^{x_i}}{Î£_j e^{x_j}} - \\frac{e^{2x_i}}{(Î£_j e^{x_j})^2}$\n",
    "\n",
    "$\\frac{d}{dx_i} softmax(x_i) = softmax(x_i) - softmax(x_i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Derivative of the Mean Squared Error Loss Function\n",
    "\n",
    "The mean squared error (MSE) loss function is commonly used in regression problems to measure the average of the squares of the errors or residuals. The MSE loss function is defined as:\n",
    "\n",
    "$MSE(y, Å·) = \\frac{Î£_i (y_i - Å·_i)^2}{n}$\n",
    "\n",
    "Where $y$ is the true value, $Å·$ is the predicted value, and $n$ is the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the derivative of the MSE loss function with respect to the predicted value $Å·_i$, we can use the chain rule of differentiation.\n",
    "\n",
    "The chain rule states that for a function $f(x) = g(h(x))$, the derivative is $f'(x) = g'(h(x)) * h'(x)$.\n",
    "\n",
    "In our case:\n",
    "\n",
    "$g(x) = (1/n) * x$\n",
    "\n",
    "$h(x) = Î£_i (y_i - Å·_i)^2$\n",
    "\n",
    "The derivative of $g(x)$ with respect to $x$ is simply $1/n$.\n",
    "\n",
    "Now, we need to find the derivative of $h(x)$ with respect to $Å·$. \n",
    "\n",
    "First, we need to expand $h(x)$:\n",
    "\n",
    "$h(x) = Î£_i (y_i - Å·_i)^2 = Î£_i (y_i^2 - 2y_iÅ·_i + Å·_i^2)$\n",
    "\n",
    "Now we can differentiate with respect to Å·_i using the power rule:\n",
    "\n",
    "$\\frac{d}{dÅ·_i} h(x) = -2y_i + 2Å·_i = 2(Å·_i - y_i)$\n",
    "\n",
    "Apply the chain rule to find the derivative of the sum:\n",
    "\n",
    "$\\frac{d}{dÅ·}  Î£_i (y_i - Å·_i)^2 = Î£_i \\frac{d}{dÅ·} (y_i - Å·_i)^2 = Î£_i 2(Å·_i - y_i)$\n",
    "\n",
    "Therefore, the derivative of the sum of squared differences with respect to Å· is:\n",
    "\n",
    "$\\frac{d}{dÅ·}  Î£_i (y_i - Å·_i)^2 = 2 Î£_i (Å·_i - y_i)$\n",
    "\n",
    "\n",
    "Finally, given:\n",
    "\n",
    "$g'(x) = (1/n)$\n",
    "\n",
    "$h'(x) = 2 Î£_i (Å·_i - y_i)$\n",
    "\n",
    "We can apply the chain rule:\n",
    "\n",
    "$\\frac{d}{dÅ·} MSE(y, Å·) = g'(h(x)) * h'(x)$\n",
    "\n",
    "$\\frac{d}{dÅ·} MSE(y, Å·) = (1/n) * 2 Î£_i (Å·_i - y_i)$\n",
    "\n",
    "$\\frac{d}{dÅ·} MSE(y, Å·) = 2/n * Î£_i (Å·_i - y_i)$\n",
    "\n",
    "This result shows that the derivative of the mean squared error is the average of the differences between the predicted values ($Å·_i$) and the true values ($y_i$), multiplied by $2/n$.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
